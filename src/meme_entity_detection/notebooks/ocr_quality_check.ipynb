{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from meme_entity_detection.dataset.data_module import DataModule\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "from spacy.cli import download\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import ast\n",
    "\n",
    "# Download the large English model if it is not already installed\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "except OSError:\n",
    "    download(\"en_core_web_lg\")\n",
    "    \n",
    "    \n",
    "nlp = spacy.load(\"en_core_web_lg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_known_words(text, nlp=nlp):   \n",
    "    text =  text.encode('utf-8', 'replace').decode() \n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Count known words\n",
    "    known_words = sum(1 for token in doc if not token.is_oov)\n",
    "    total_words = len(doc)\n",
    "    \n",
    "    return known_words, total_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"../../../data/HVVMemes/\")\n",
    "dataset = DataModule(data_dir, balance_train_dataset=False)\n",
    "dataset.setup(\"_\")\n",
    "clear_output(wait=False)\n",
    "\n",
    "train_df = dataset.train_dataset.data_df\n",
    "validation_df = dataset.validation_dataset.data_df\n",
    "test_df  = dataset.test_dataset.data_df\n",
    "\n",
    "# Concatenating all dataframes to get a combined view\n",
    "combined_df = pd.concat([train_df.assign(dataset='train'), \n",
    "                         validation_df.assign(dataset='validation'), \n",
    "                         test_df.assign(dataset='test')])[[\"sentence\", \"original\", \"dataset\", \"image\", \"word\"]].drop_duplicates()\n",
    "\n",
    "combined_df[\"image_path\"] = str(data_dir) + \"/images/\" + combined_df[\"image\"]\n",
    "\n",
    "entities_in_image = combined_df.groupby(\"image\")[\"word\"].apply(list).reset_index()\n",
    "combined_df = combined_df[[\"sentence\", \"original\", \"dataset\", \"image\", \"image_path\"]].drop_duplicates()\n",
    "combined_df = combined_df.merge(entities_in_image, on=\"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for OCR Quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct words ration of existing data:  92.87 %\n"
     ]
    }
   ],
   "source": [
    "word_counter = combined_df[\"sentence\"].apply(count_known_words)\n",
    "combined_df[\"correct words\"] = word_counter.apply(lambda x: x[0])\n",
    "combined_df[\"all words\"] = word_counter.apply(lambda x: x[1])\n",
    "\n",
    "correct_words_ratio = (combined_df[\"correct words\"].sum() / combined_df[\"all words\"].sum())\n",
    "print(f\"Correct words ration of existing data:  {round(correct_words_ratio*100, 2)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT API Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = input(\"Please insert your api key:\")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path: list):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "  \n",
    "def build_prompt(entities):\n",
    "  entities_dict = {entity: \"fill in the class here\" for entity in entities}\n",
    "\n",
    "\n",
    "  json_format = \"\"\"{\"OCR\": \"OCR OF THE IMAGE\",\n",
    "  \"IMAGE_DESCRIPTION: \"DESCRIPTION_OF_IMAGE_CONTENTS\",\n",
    "  \"CLASSIFICATION\": \"\"\"+str(entities_dict).replace('\",', '\",\\n') + \"\"\"\n",
    "  }\"\"\"\n",
    "                              \n",
    "  text = f\"\"\"Process the image and fill the following json-object in the follwing schema. \n",
    "  You should OCR, image description and classification. Do net return anything else than the json.\n",
    "  Do not change the format of the json. If you can not fill in the json, return a None inside of the json:\n",
    "                      \n",
    "  For the classification note that each entitly must exactly have one class.\n",
    "  Do not change the name of the entities, even if they are misspelled.\n",
    "                      \n",
    "  The classes you can choose from are:\n",
    "  \"other\", \"villian\", \"victim\", \"hero\"\n",
    "                      \n",
    "                      \n",
    "  Here is the json template:\n",
    "                      \n",
    "  {json_format}\n",
    "                              \n",
    "                      \n",
    "  \"\"\"\n",
    "  \n",
    "  return text\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_api_call(image: base64, entities: list):\n",
    "  \n",
    "  client = OpenAI(api_key=api_key)\n",
    "\n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": build_prompt(entities)\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "              \"url\": f\"data:image/jpeg;base64,{image}\"\n",
    "            }\n",
    "          \n",
    "          },\n",
    "        ],\n",
    "      }\n",
    "    ],\n",
    "    max_tokens=800,\n",
    "  )\n",
    "      \n",
    "  return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6716it [7:26:26,  3.99s/it]\n"
     ]
    }
   ],
   "source": [
    "json_path = data_dir.parent / f'ocr_data_openai.json'\n",
    "\n",
    "ocr_data = json.loads(open(json_path, \"r\").read())\n",
    "\n",
    "for sentence, entities, image, image_path in tqdm(zip(combined_df[\"sentence\"].tolist(), combined_df[\"word\"], \n",
    "                                            combined_df[\"image\"].tolist(), combined_df[\"image_path\"].tolist())):  #\n",
    "    \n",
    "    if not image in ocr_data.keys(): \n",
    "        \n",
    "        encoded_image = encode_image(image_path)\n",
    "        try:\n",
    "            content = send_api_call(encoded_image, entities)\n",
    "            ocr_data[image] = content\n",
    "        except:\n",
    "            ocr_data[image] = \"API Error\"\n",
    "            \n",
    "            # Save the OCR data after each iteration\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(ocr_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6716/6716 [00:00<00:00, 64601.15it/s]\n"
     ]
    }
   ],
   "source": [
    "ocr_data_cleaned = dict()\n",
    "error_data = dict()\n",
    "for k, v in tqdm(ocr_data.items()):\n",
    "    try:\n",
    "        keys_filtered_a = \"{\"+\"{\".join(ocr_data[k].split(\"{\")[1:])\n",
    "        keys_filtered_b = (\"\".join(keys_filtered_a.split(\"}\")[:-1]) + \"}\" + \"}\").replace('\"IMAGE_DESCRIPTION:', '\"IMAGE_DESCRIPTION\":')\n",
    "        ocr_data_cleaned[k] = dict_obj = ast.literal_eval(keys_filtered_b)\n",
    "    except:\n",
    "        error_data[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [05:44,  4.30s/it]\n"
     ]
    }
   ],
   "source": [
    "error_df = combined_df[combined_df[\"image\"].isin(error_data)]\n",
    "\n",
    "for sentence, entities, image, image_path in tqdm(zip(error_df[\"sentence\"].tolist(), error_df[\"word\"], \n",
    "                                            error_df[\"image\"].tolist(), error_df[\"image_path\"].tolist())):  #\n",
    "    \n",
    "    encoded_image = encode_image(image_path)\n",
    "    try:\n",
    "        content = send_api_call(encoded_image, entities)\n",
    "        ocr_data[image] = content\n",
    "    except:\n",
    "        ocr_data[image] = \"API Error\"\n",
    "            \n",
    "            # Save the OCR data after each iteration\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(ocr_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6716/6716 [00:00<00:00, 58192.73it/s]\n"
     ]
    }
   ],
   "source": [
    "ocr_data_cleaned = dict()\n",
    "error_data = dict()\n",
    "\n",
    "for k, v in tqdm(ocr_data.items()):\n",
    "    try:\n",
    "        keys_filtered_a = \"{\"+\"{\".join(ocr_data[k].split(\"{\")[1:])\n",
    "        keys_filtered_b = (\"\".join(keys_filtered_a.split(\"}\")[:-1]) + \"}\" + \"}\").replace('\"IMAGE_DESCRIPTION:', '\"IMAGE_DESCRIPTION\":')\n",
    "        ocr_data_cleaned[k] = dict_obj = ast.literal_eval(keys_filtered_b)\n",
    "    except:\n",
    "        error_data[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path_filtered = data_dir.parent / f'ocr_data_openai_preprocessed.json'\n",
    "\n",
    "with open(json_path_filtered, 'w') as f:\n",
    "    json.dump(ocr_data_cleaned, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct words ration of ocr preprocessed data:  90.0 %\n"
     ]
    }
   ],
   "source": [
    "df_openai = pd.DataFrame(ocr_data_cleaned).T\n",
    "df_openai = df_openai.dropna()\n",
    "\n",
    "word_counter = df_openai[\"OCR\"].apply(count_known_words)\n",
    "df_openai[\"correct words\"] = word_counter.apply(lambda x: x[0])\n",
    "df_openai[\"all words\"] = word_counter.apply(lambda x: x[1])\n",
    "\n",
    "correct_words_ratio = (df_openai[\"correct words\"].sum() / df_openai[\"all words\"].sum())\n",
    "print(f\"Correct words ration of ocr preprocessed data:  {round(correct_words_ratio*100, 2)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8999824980310285"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_words_ratio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

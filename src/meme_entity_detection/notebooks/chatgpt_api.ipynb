{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from en-core-web-lg==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.8.0)\n",
      "Requirement already satisfied: jinja2 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (70.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.0 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.6.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/.cache/pypoetry/virtualenvs/meme-entity-detection-m8bB3Wlx-py3.10/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from meme_entity_detection.dataset.data_module import DataModule\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "from spacy.cli import download\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import ast\n",
    "\n",
    "# Download the large English model if it is not already installed\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "except OSError:\n",
    "    download(\"en_core_web_lg\")\n",
    "    \n",
    "    \n",
    "nlp = spacy.load(\"en_core_web_lg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import IPython.display\n",
    "import pandas as pd\n",
    "\n",
    "import meme_entity_detection.dataset\n",
    "\n",
    "data_dir = Path(\"../../../data/HVVMemes/\")\n",
    "dataset = meme_entity_detection.dataset.DataModule(data_dir, balance_train_dataset=False)\n",
    "dataset.setup(\"_\")\n",
    "IPython.display.clear_output(wait=False)\n",
    "\n",
    "train_df = dataset.train_dataset.data_df\n",
    "validation_df = dataset.validation_dataset.data_df\n",
    "test_df = dataset.test_dataset.data_df\n",
    "\n",
    "# Concatenating all dataframes to get a combined view\n",
    "combined_df = pd.concat([\n",
    "    train_df.assign(dataset='train'),\n",
    "    validation_df.assign(dataset='validation'),\n",
    "    test_df.assign(dataset='test')\n",
    "])[[\"sentence\", \"original\", \"dataset\", \"image\", \"word\"]].drop_duplicates()\n",
    "\n",
    "combined_df[\"image_path\"] = str(data_dir) + \"/images/\" + combined_df[\"image\"]\n",
    "\n",
    "entities_in_image = combined_df.groupby(\"image\")[\"word\"].apply(list).reset_index()\n",
    "combined_df = combined_df[[\"sentence\", \"original\", \"dataset\", \"image\", \"image_path\"]].drop_duplicates()\n",
    "combined_df = combined_df.merge(entities_in_image, on=\"image\")\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for OCR Quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct words ration of existing data:  92.87 %\n"
     ]
    }
   ],
   "source": [
    "word_counter = combined_df[\"sentence\"].apply(count_known_words)\n",
    "combined_df[\"correct words\"] = word_counter.apply(lambda x: x[0])\n",
    "combined_df[\"all words\"] = word_counter.apply(lambda x: x[1])\n",
    "\n",
    "correct_words_ratio = (combined_df[\"correct words\"].sum() / combined_df[\"all words\"].sum())\n",
    "print(f\"Correct words ration of existing data:  {round(correct_words_ratio*100, 2)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT API Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = input(\"Please insert your api key:\")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path: list):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "  \n",
    "def build_prompt(entities):\n",
    "  entities_dict = {entity: \"fill in the class here\" for entity in entities}\n",
    "\n",
    "\n",
    "  json_format = \"\"\"{\"OCR\": \"OCR OF THE IMAGE\",\n",
    "  \"IMAGE_DESCRIPTION: \"DESCRIPTION_OF_IMAGE_CONTENTS\",\n",
    "  \"CLASSIFICATION\": \"\"\"+str(entities_dict).replace('\",', '\",\\n') + \"\"\"\n",
    "  }\"\"\"\n",
    "                              \n",
    "  text = f\"\"\"Process the image and fill the following json-object in the follwing schema. \n",
    "  You should OCR, image description and classification. Do net return anything else than the json.\n",
    "  Do not change the format of the json. If you can not fill in the json, return a None inside of the json:\n",
    "                      \n",
    "  For the classification note that each entitly must exactly have one class.\n",
    "  Do not change the name of the entities, even if they are misspelled.\n",
    "                      \n",
    "  The classes you can choose from are:\n",
    "  \"other\", \"villian\", \"victim\", \"hero\"\n",
    "                      \n",
    "                      \n",
    "  Here is the json template:\n",
    "                      \n",
    "  {json_format}\n",
    "                              \n",
    "                      \n",
    "  \"\"\"\n",
    "  \n",
    "  return text\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_api_call(image: base64, entities: list):\n",
    "  \n",
    "  client = OpenAI(api_key=api_key)\n",
    "\n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": build_prompt(entities)\n",
    "          },\n",
    "          {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "              \"url\": f\"data:image/jpeg;base64,{image}\"\n",
    "            }\n",
    "          \n",
    "          },\n",
    "        ],\n",
    "      }\n",
    "    ],\n",
    "    max_tokens=800,\n",
    "  )\n",
    "      \n",
    "  return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6716it [7:26:26,  3.99s/it]\n"
     ]
    }
   ],
   "source": [
    "json_path = data_dir.parent / f'ocr_data_openai.json'\n",
    "\n",
    "ocr_data = json.loads(open(json_path, \"r\").read())\n",
    "\n",
    "for sentence, entities, image, image_path in tqdm(zip(combined_df[\"sentence\"].tolist(), combined_df[\"word\"], \n",
    "                                            combined_df[\"image\"].tolist(), combined_df[\"image_path\"].tolist())):  #\n",
    "    \n",
    "    if not image in ocr_data.keys(): \n",
    "        \n",
    "        encoded_image = encode_image(image_path)\n",
    "        try:\n",
    "            content = send_api_call(encoded_image, entities)\n",
    "            ocr_data[image] = content\n",
    "        except:\n",
    "            ocr_data[image] = \"API Error\"\n",
    "            \n",
    "            # Save the OCR data after each iteration\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(ocr_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6716/6716 [00:00<00:00, 64601.15it/s]\n"
     ]
    }
   ],
   "source": [
    "ocr_data_cleaned = dict()\n",
    "error_data = dict()\n",
    "for k, v in tqdm(ocr_data.items()):\n",
    "    try:\n",
    "        keys_filtered_a = \"{\"+\"{\".join(ocr_data[k].split(\"{\")[1:])\n",
    "        keys_filtered_b = (\"\".join(keys_filtered_a.split(\"}\")[:-1]) + \"}\" + \"}\").replace('\"IMAGE_DESCRIPTION:', '\"IMAGE_DESCRIPTION\":')\n",
    "        ocr_data_cleaned[k] = dict_obj = ast.literal_eval(keys_filtered_b)\n",
    "    except:\n",
    "        error_data[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [05:44,  4.30s/it]\n"
     ]
    }
   ],
   "source": [
    "error_df = combined_df[combined_df[\"image\"].isin(error_data)]\n",
    "\n",
    "for sentence, entities, image, image_path in tqdm(zip(error_df[\"sentence\"].tolist(), error_df[\"word\"], \n",
    "                                            error_df[\"image\"].tolist(), error_df[\"image_path\"].tolist())):  #\n",
    "    \n",
    "    encoded_image = encode_image(image_path)\n",
    "    try:\n",
    "        content = send_api_call(encoded_image, entities)\n",
    "        ocr_data[image] = content\n",
    "    except:\n",
    "        ocr_data[image] = \"API Error\"\n",
    "            \n",
    "            # Save the OCR data after each iteration\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(ocr_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6716/6716 [00:00<00:00, 58192.73it/s]\n"
     ]
    }
   ],
   "source": [
    "ocr_data_cleaned = dict()\n",
    "error_data = dict()\n",
    "\n",
    "for k, v in tqdm(ocr_data.items()):\n",
    "    try:\n",
    "        keys_filtered_a = \"{\"+\"{\".join(ocr_data[k].split(\"{\")[1:])\n",
    "        keys_filtered_b = (\"\".join(keys_filtered_a.split(\"}\")[:-1]) + \"}\" + \"}\").replace('\"IMAGE_DESCRIPTION:', '\"IMAGE_DESCRIPTION\":')\n",
    "        ocr_data_cleaned[k] = dict_obj = ast.literal_eval(keys_filtered_b)\n",
    "    except:\n",
    "        error_data[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path_filtered = data_dir.parent / f'ocr_data_openai_preprocessed.json'\n",
    "\n",
    "with open(json_path_filtered, 'w') as f:\n",
    "    json.dump(ocr_data_cleaned, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
